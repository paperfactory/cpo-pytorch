{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path):\n",
    "    ''' Load data from train.csv or test.csv. '''\n",
    "\n",
    "    data = pd.read_csv(data_path, sep=';')\n",
    "    for col in ['state', 'n_state', 'action_reward']:\n",
    "        data[col] = [np.array([[np.int(k) for k in ee.split('&')] for ee in e.split('|')]) for e in data[col]]\n",
    "    for col in ['state', 'n_state']:\n",
    "        data[col] = [np.array([e[0] for e in l]) for l in data[col]]\n",
    "\n",
    "    data['action'] = [[e[0] for e in l] for l in data['action_reward']]\n",
    "    data['reward'] = [tuple(e[1] for e in l) for l in data['action_reward']]\n",
    "    data.drop(columns=['action_reward'], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    ''' Load embeddings (a vector for each item). '''\n",
    "\n",
    "    embeddings = pd.read_csv(embeddings_path, sep=';')\n",
    "\n",
    "    return np.array([[np.float64(k) for k in e.split('|')]\n",
    "                   for e in embeddings['vectors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file('train.csv')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, item_embeddings):\n",
    "        self.item_embeddings = item_embeddings\n",
    "\n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "\n",
    "    def get_embedding(self, item_index):\n",
    "        return self.item_embeddings[item_index]\n",
    "\n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embedding(item) for item in item_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(read_embeddings('embeddings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "class Fairrec(gym.Env):\n",
    "    def __init__(self, data, embeddings, alpha, gamma, fixed_length, state_size, trajectory_length):\n",
    "\n",
    "        self.embedding_len = embeddings.size()\n",
    "        self.embeddings = embeddings\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [row['state'] for _, row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [row['action'] for _, row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "        self.embedded_data['state_embed'] = [np.array([embeddings.get_embedding(item_id) \n",
    "\t\t\tfor item_id in row['state']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['action_embed'] = [np.array([embeddings.get_embedding(item_id) \n",
    "\t\t\tfor item_id in row['action']]) for _, row in data.iterrows()]\n",
    "\n",
    "        self.alpha = alpha # α (alpha) in Equation (1)\n",
    "        self.gamma = gamma # Γ (Gamma) in Equation (4)\n",
    "        self.fixed_length = True\n",
    "        self.current_state = self.reset()\n",
    "        # print(self.current_state)\n",
    "        self.groups = self.get_groups()\n",
    "\n",
    "        self.action_space = spaces.Box(low=-10, high=10, shape=(self.embedding_len,))\n",
    "        self.observation_space = spaces.Box(low=-10, high=10, shape=(state_size*self.embedding_len,))\n",
    "        self.counter = 1\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.done = False\n",
    "\n",
    "        print('Fair-Rec Environment initialized')\n",
    "\n",
    "    # \tdef set_observation_space(self, state_size):\n",
    "    # \t\tself.observation_space = spaces.Box(low=-1, high=1, shape=(state_size*fixed_length,))\n",
    "\n",
    "    # \tdef set_action_space(self, action_size):\n",
    "    # \t\tself.action_space = spaces.Box(low=-1, high=1, shape=(action_size*fixed_length,))\n",
    "\n",
    "    def reset(self):\n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "        return self.init_state\n",
    "\n",
    "    def step(self, actions):\n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "        self.counter += 1\n",
    "        \n",
    "        current_state_embeds = np.array(self.embeddings.embed(self.current_state))\n",
    "        action_embeds = np.array(self.embeddings.embed(actions))\n",
    "\n",
    "        # '18: Compute overall reward r_t according to Equation (4)'\n",
    "        simulated_rewards, cumulated_reward = self.simulate_rewards(current_state_embeds.reshape((1, -1)), action_embeds.reshape((1, -1)))\n",
    "\n",
    "        # '11: Set s_t+1 = s_t' <=> self.current_state = self.current_state\n",
    "\n",
    "        for k in range(len(simulated_rewards)): # '12: for k = 1, K do'\n",
    "            if simulated_rewards[k] > 0: # '13: if r_t^k > 0 then'\n",
    "                # print(simulated_rewards[k])\n",
    "            # '14: Add a_t^k to the end of s_t+1'\n",
    "                self.current_state = np.append(self.current_state, [actions[k]], axis=0)\n",
    "                if self.fixed_length: # '15: Remove the first item of s_t+1'\n",
    "                    self.current_state = np.delete(self.current_state, 0, axis=0)\n",
    "\n",
    "        if self.counter > self.trajectory_length:\n",
    "        \tself.done = True\n",
    "        \n",
    "        return cumulated_reward, self.current_state, self.done\n",
    "\n",
    "    def get_groups(self):\n",
    "        ''' Calculate average state/action value for each group. Equation (3). '''\n",
    "\n",
    "        groups = []\n",
    "        for rewards, group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state_embed'].values))\n",
    "            actions = np.array(list(group['action_embed'].values))\n",
    "            groups.append({\n",
    "            'size': size, # N_x in article\n",
    "            'rewards': rewards, # U_x in article (combination of rewards)\n",
    "            'average state': (np.sum(states / np.linalg.norm(states, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)), # s_x^-\n",
    "            'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)) # a_x^-\n",
    "            })\n",
    "        return groups\n",
    "\n",
    "    def simulate_rewards(self, current_state, chosen_actions, reward_type='grouped cosine'):\n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          chosen_actions: embedded chosen items.\n",
    "          reward_type: from ['normal', 'grouped average', 'grouped cosine'].\n",
    "        Returns:\n",
    "          returned_rewards: most probable rewards.\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "        \n",
    "        # Equation (1)\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "            return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "\n",
    "        if reward_type == 'normal':\n",
    "            # Calculate simulated reward in normal way: Equation (2)\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, row['state'], row['action'])\n",
    "            for _, row in self.embedded_data.iterrows()]\n",
    "        elif reward_type == 'grouped average':\n",
    "            # Calculate simulated reward by grouped average: Equation (3)\n",
    "            probabilities = np.array([g['size'] for g in self.groups]) *\\\n",
    "            [(self.alpha * (np.dot(current_state, g['average state'].T) / np.linalg.norm(current_state, 2))\\\n",
    "            + (1 - self.alpha) * (np.dot(chosen_actions, g['average action'].T) / np.linalg.norm(chosen_actions, 2)))\n",
    "            for g in self.groups]\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            # Calculate simulated reward by grouped cosine: Equations (1) and (3)\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, g['average state'], g['average action'])\n",
    "            for g in self.groups]\n",
    "\n",
    "        # Normalize (sum to 1)\n",
    "        probabilities = np.array(probabilities) / sum(probabilities)\n",
    "\n",
    "        # Get most probable rewards\n",
    "        if reward_type == 'normal':\n",
    "            returned_rewards = self.embedded_data.iloc[np.argmax(probabilities)]['reward']\n",
    "        elif reward_type in ['grouped average', 'grouped cosine']:\n",
    "            returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "\n",
    "        # Equation (4)\n",
    "        def overall_reward(rewards, gamma):\n",
    "            return np.sum([gamma**k * reward for k, reward in enumerate(rewards)])\n",
    "\n",
    "        if reward_type in ['normal', 'grouped average']:\n",
    "            # Get cumulated reward: Equation (4)\n",
    "            cumulated_reward = overall_reward(returned_rewards, self.gamma)\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            # Get probability weighted cumulated reward\n",
    "            cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma)\n",
    "            for p, g in zip(probabilities, self.groups)])\n",
    "\n",
    "        return returned_rewards, cumulated_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair-Rec Environment initialized\n"
     ]
    }
   ],
   "source": [
    "env = Fairrec(data=data, embeddings=embeddings, alpha=0.5, gamma=0.9, fixed_length=False, state_size=12, trajectory_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.850312981840485,\n",
       " array([   1,    2,    3, 1223,    1,    2,    3, 1223,    1,    2,    3,\n",
       "        1223]),\n",
       " True)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([1,2,3,1223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.preprocess(data, embeddings, 0.5, 0.9, True)\n",
    "# env = gym.make('fairrec-v0',data=data, embeddings=embeddings, alpha=0.5, gamma=0.9, fixed_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 249,  455,  301,  295,  117,  286, 1013,  274,  291,  748,  282,\n",
       "        815])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = embeddings.get_embedding([1,2,3,4])\n",
    "# actions = embeddings.embed([1,2,3,4])\n",
    "# actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1121,  686,  135,  492, 1203,  481,  216,  524,   23,  705,  217,\n",
       "       1050])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.embedded_data['state'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = torch.from_numpy(embeddings.get_embedding_vector()).expand(4,-1,100)\n",
    "actions = torch.from_numpy(embeddings.get_embedding([1,2,3,4])).view(4,1,100)#.view(4,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 100]), torch.Size([4, 1682, 100]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape,item_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1682, 100])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.mul(item_embeddings,actions)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4]) torch.Size([1682, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1682, 4])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings = torch.from_numpy(embeddings.get_embedding_vector())\n",
    "actions = torch.from_numpy(embeddings.get_embedding([1,2,3,4])).view(100,-1)\n",
    "print(actions.shape,item_embeddings.shape)\n",
    "res = torch.mm(item_embeddings,actions)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.]],\n",
       "\n",
       "         [[10., 20., 30.],\n",
       "          [40., 50., 60.]]],\n",
       "\n",
       "\n",
       "        [[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.]],\n",
       "\n",
       "         [[10., 20., 30.],\n",
       "          [40., 50., 60.]]],\n",
       "\n",
       "\n",
       "        [[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.]],\n",
       "\n",
       "         [[10., 20., 30.],\n",
       "          [40., 50., 60.]]],\n",
       "\n",
       "\n",
       "        [[[ 1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.]],\n",
       "\n",
       "         [[10., 20., 30.],\n",
       "          [40., 50., 60.]]]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.Tensor([[[1, 2, 3], [4, 5, 6]],[[10, 20, 30], [40, 50, 60]]])\n",
    "M.expand(4,2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "x[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  4.],\n",
       "         [ 2.,  5.],\n",
       "         [ 3.,  6.]],\n",
       "\n",
       "        [[10., 40.],\n",
       "         [20., 50.],\n",
       "         [30., 60.]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(M,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,400,100):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 287, 1293,  741,  160])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[ 0.6172, -0.0315,  0.2512,  0.1234],\n",
       "        [ 0.5960, -0.0460,  0.2373,  0.1164],\n",
       "        [ 0.5311, -0.0733,  0.2235,  0.1156],\n",
       "        [ 0.5309, -0.0995,  0.2215,  0.1144]], dtype=torch.float64),\n",
       "indices=tensor([[ 287, 1293,  741,  160],\n",
       "        [ 244, 1463,  463, 1581],\n",
       "        [1366,  568, 1292, 1627],\n",
       "        [1296,  316,  475, 1490]]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(res, k=4, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fairrec(gym.Env):\n",
    "    def __init__(self, data, embeddings, alpha, gamma, fixed_length):\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(100,))\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(12*100,))\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id) \n",
    "            for item_id in row['state']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id) \n",
    "            for item_id in row['action']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "\n",
    "        self.alpha = alpha # α (alpha) in Equation (1)\n",
    "        self.gamma = gamma # Γ (Gamma) in Equation (4)\n",
    "        self.fixed_length = True\n",
    "        self.current_state = self.reset()\n",
    "        self.groups = self.get_groups()\n",
    "\n",
    "        print('Fair-Rec Environment initialized')\n",
    "\n",
    "    def set_observation_space(self, state_size):\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(state_size*100,))\n",
    "\n",
    "    def set_action_space(self, action_size):\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size*100,))\n",
    "\n",
    "    def reset(self):\n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "        return self.init_state\n",
    "\n",
    "    def step(self, actions):\n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "\n",
    "        # '18: Compute overall reward r_t according to Equation (4)'\n",
    "        simulated_rewards, cumulated_reward = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
    "\n",
    "        # '11: Set s_t+1 = s_t' <=> self.current_state = self.current_state\n",
    "\n",
    "        for k in range(len(simulated_rewards)): # '12: for k = 1, K do'\n",
    "            if simulated_rewards[k] > 0: # '13: if r_t^k > 0 then'\n",
    "                # '14: Add a_t^k to the end of s_t+1'\n",
    "                print(simulated_rewards[k])\n",
    "                self.current_state = np.append(self.current_state, [actions[k]], axis=0)\n",
    "                if self.fixed_length: # '15: Remove the first item of s_t+1'\n",
    "                    self.current_state = np.delete(self.current_state, 0, axis=0)\n",
    "\n",
    "        return cumulated_reward, self.current_state\n",
    "\n",
    "    def get_groups(self):\n",
    "        ''' Calculate average state/action value for each group. Equation (3). '''\n",
    "\n",
    "        groups = []\n",
    "        for rewards, group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state'].values))\n",
    "            actions = np.array(list(group['action'].values))\n",
    "            groups.append({\n",
    "            'size': size, # N_x in article\n",
    "            'rewards': rewards, # U_x in article (combination of rewards)\n",
    "            'average state': (np.sum(states / np.linalg.norm(states, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)), # s_x^-\n",
    "            'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis=1)[:, np.newaxis], axis=0) / size).reshape((1, -1)) # a_x^-\n",
    "            })\n",
    "        return groups\n",
    "\n",
    "    def simulate_rewards(self, current_state, chosen_actions, reward_type='grouped cosine'):\n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          chosen_actions: embedded chosen items.\n",
    "          reward_type: from ['normal', 'grouped average', 'grouped cosine'].\n",
    "        Returns:\n",
    "          returned_rewards: most probable rewards.\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "\n",
    "        # Equation (1)\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "            return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "\n",
    "        if reward_type == 'normal':\n",
    "            # Calculate simulated reward in normal way: Equation (2)\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, row['state'], row['action'])\n",
    "            for _, row in self.embedded_data.iterrows()]\n",
    "        elif reward_type == 'grouped average':\n",
    "            # Calculate simulated reward by grouped average: Equation (3)\n",
    "            probabilities = np.array([g['size'] for g in self.groups]) *\\\n",
    "            [(self.alpha * (np.dot(current_state, g['average state'].T) / np.linalg.norm(current_state, 2))\\\n",
    "            + (1 - self.alpha) * (np.dot(chosen_actions, g['average action'].T) / np.linalg.norm(chosen_actions, 2)))\n",
    "            for g in self.groups]\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            # Calculate simulated reward by grouped cosine: Equations (1) and (3)\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, g['average state'], g['average action'])\n",
    "            for g in self.groups]\n",
    "\n",
    "        # Normalize (sum to 1)\n",
    "        probabilities = np.array(probabilities) / sum(probabilities)\n",
    "\n",
    "        # Get most probable rewards\n",
    "        if reward_type == 'normal':\n",
    "            returned_rewards = self.embedded_data.iloc[np.argmax(probabilities)]['reward']\n",
    "        elif reward_type in ['grouped average', 'grouped cosine']:\n",
    "            returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "\n",
    "        # Equation (4)\n",
    "        def overall_reward(rewards, gamma):\n",
    "            return np.sum([gamma**k * reward for k, reward in enumerate(rewards)])\n",
    "\n",
    "        if reward_type in ['normal', 'grouped average']:\n",
    "            # Get cumulated reward: Equation (4)\n",
    "            cumulated_reward = overall_reward(returned_rewards, self.gamma)\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            # Get probability weighted cumulated reward\n",
    "            cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma)\n",
    "            for p, g in zip(probabilities, self.groups)])\n",
    "\n",
    "        return returned_rewards, cumulated_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair-Rec Environment initialized\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair-Rec Environment initialized\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.850470800900226,\n",
       " array([[-0.18108723,  0.04148261, -0.11469268, ...,  0.146907  ,\n",
       "         -0.10624914,  0.15694171],\n",
       "        [-0.24884087,  0.1280875 , -0.10616239, ...,  0.1545012 ,\n",
       "         -0.11269389,  0.18657199],\n",
       "        [-0.15192594,  0.00835354, -0.18226244, ...,  0.20323972,\n",
       "         -0.12260215,  0.15912019],\n",
       "        ...,\n",
       "        [-0.20948793,  0.05360457, -0.10730472, ...,  0.07926252,\n",
       "         -0.07703452,  0.03268938],\n",
       "        [-0.20113458,  0.06713491, -0.14161727, ...,  0.20752539,\n",
       "         -0.12113315,  0.14045045],\n",
       "        [-0.15918495,  0.0930948 , -0.1697659 , ...,  0.13854662,\n",
       "         -0.15439157,  0.06976631]]))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Fairrec(data=data, embeddings=embeddings, alpha=0.5, gamma=0.9, fixed_length=True)\n",
    "actions = embeddings.get_embedding([1,2,3,4])\n",
    "env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['state'][0]) - len(data['n_state'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3, 4)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['reward'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
